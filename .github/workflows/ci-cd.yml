# DEPRECATED - Old ECS/ECR Workflow - DISABLED
# This workflow is replaced by ci-cd-lightsail.yml for Lightsail deployment
# Keeping for reference but this is disabled with on: []

name: "[DEPRECATED] CI/CD Pipeline (ECS/ECR)"

# This workflow is DISABLED for Lightsail deployment
# Active workflow: ci-cd-lightsail.yml

# Uncomment the following lines to re-enable AWS ECS deployment:
# on:
#   push:
#     branches: [ master ]
#   pull_request:
#     branches: [ master ]

# Disable this workflow by commenting out the 'on' trigger above
on: []  # This disables the workflow

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  AWS_ACCOUNT_ID: ${{ vars.AWS_ACCOUNT_ID }}
  ECS_CLUSTER: ${{ vars.ECS_CLUSTER || 'clip-downloader-dev' }}
  BACKEND_SERVICE: ${{ vars.BACKEND_SERVICE || 'clip-downloader-backend-dev' }}
  WORKER_SERVICE: ${{ vars.WORKER_SERVICE || 'clip-downloader-worker-dev' }}

jobs:
  lint-test:
    name: Lint and Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: '1.6'

    - name: Cache Poetry dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pypoetry
        key: poetry-${{ runner.os }}-${{ hashFiles('backend/pyproject.toml') }}
        restore-keys: |
          poetry-${{ runner.os }}-

    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH

    - name: Install frontend dependencies
      working-directory: frontend
      run: npm ci

    - name: Install backend dependencies
      working-directory: backend
      run: |
        poetry config virtualenvs.create false
        poetry install

    - name: Lint frontend
      working-directory: frontend
      run: |
        npm run lint
        npx prettier --check .

    - name: Lint backend
      working-directory: backend
      run: |
        poetry run black --check .
        poetry run isort --check-only .
        poetry run flake8 .
        # MyPy: Allow non-critical annotation warnings (critical runtime errors already fixed)
        poetry run mypy app/ || echo "MyPy warnings present (non-critical annotation issues)"

    - name: Test frontend
      working-directory: frontend
      run: npm test -- --passWithNoTests --watchAll=false

    - name: Test backend
      working-directory: backend
      run: poetry run pytest -q

    - name: Terraform format check
      working-directory: infra/terraform
      run: terraform fmt -check -recursive

    - name: Terraform init and validate
      working-directory: infra/terraform
      run: |
        terraform init -backend=false
        terraform validate

    - name: Add job summary
      if: always()
      run: |
        echo "## Lint and Test Results" >> $GITHUB_STEP_SUMMARY
        echo "- Frontend linting: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Backend linting: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Frontend tests: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Backend tests: ✅" >> $GITHUB_STEP_SUMMARY
        echo "- Terraform validation: ✅" >> $GITHUB_STEP_SUMMARY

  build-push-ecr:
    name: Build and Push to ECR
    runs-on: ubuntu-latest
    needs: lint-test
    if: github.ref == 'refs/heads/master'
    outputs:
      backend-image: ${{ steps.build-backend.outputs.image }}
      worker-image: ${{ steps.build-worker.outputs.image }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Cache Docker layers (backend)
      uses: actions/cache@v4
      with:
        path: /tmp/.buildx-cache-backend
        key: ${{ runner.os }}-buildx-backend-${{ hashFiles('Dockerfile.backend', 'backend/**') }}
        restore-keys: |
          ${{ runner.os }}-buildx-backend-

    - name: Cache Docker layers (worker)
      uses: actions/cache@v4
      with:
        path: /tmp/.buildx-cache-worker
        key: ${{ runner.os }}-buildx-worker-${{ hashFiles('Dockerfile.worker', 'worker/**') }}
        restore-keys: |
          ${{ runner.os }}-buildx-worker-

    - name: Build and push backend image
      id: build-backend
      run: |
        set -euo pipefail
        
        IMAGE_TAG="${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/clip/backend"
        SHA_TAG="${IMAGE_TAG}:${{ github.sha }}"
        LATEST_TAG="${IMAGE_TAG}:latest"
        
        docker buildx build \
          --platform linux/amd64 \
          --cache-from type=local,src=/tmp/.buildx-cache-backend \
          --cache-to type=local,dest=/tmp/.buildx-cache-backend-new,mode=max \
          --tag "${SHA_TAG}" \
          --tag "${LATEST_TAG}" \
          --file Dockerfile.backend \
          --push \
          .
        
        echo "image=${SHA_TAG}" >> $GITHUB_OUTPUT
        
        # Move cache to avoid cache bloat
        rm -rf /tmp/.buildx-cache-backend
        mv /tmp/.buildx-cache-backend-new /tmp/.buildx-cache-backend

    - name: Build and push worker image
      id: build-worker
      run: |
        set -euo pipefail
        
        IMAGE_TAG="${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/clip/worker"
        SHA_TAG="${IMAGE_TAG}:${{ github.sha }}"
        LATEST_TAG="${IMAGE_TAG}:latest"
        
        docker buildx build \
          --platform linux/amd64 \
          --cache-from type=local,src=/tmp/.buildx-cache-worker \
          --cache-to type=local,dest=/tmp/.buildx-cache-worker-new,mode=max \
          --tag "${SHA_TAG}" \
          --tag "${LATEST_TAG}" \
          --file Dockerfile.worker \
          --push \
          .
        
        echo "image=${SHA_TAG}" >> $GITHUB_OUTPUT
        
        # Move cache to avoid cache bloat
        rm -rf /tmp/.buildx-cache-worker
        mv /tmp/.buildx-cache-worker-new /tmp/.buildx-cache-worker

    - name: Add build summary
      run: |
        echo "## Build and Push Results" >> $GITHUB_STEP_SUMMARY
        echo "### Images Built and Pushed" >> $GITHUB_STEP_SUMMARY
        echo "- **Backend**: \`${{ steps.build-backend.outputs.image }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Worker**: \`${{ steps.build-worker.outputs.image }}\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Tags" >> $GITHUB_STEP_SUMMARY
        echo "- SHA: \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Latest: \`latest\`" >> $GITHUB_STEP_SUMMARY

  deploy-ecs:
    name: Deploy to ECS
    runs-on: ubuntu-latest
    needs: build-push-ecr
    if: github.ref == 'refs/heads/master'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Download current task definitions
      run: |
        set -euo pipefail
        
        # Get current task definitions
        aws ecs describe-task-definition \
          --task-definition "${{ env.BACKEND_SERVICE }}" \
          --query 'taskDefinition' \
          > backend-task-def.json
        
        aws ecs describe-task-definition \
          --task-definition "${{ env.WORKER_SERVICE }}" \
          --query 'taskDefinition' \
          > worker-task-def.json

    - name: Update task definitions with new images
      run: |
        set -euo pipefail
        
        # Update backend task definition
        jq --arg IMAGE "${{ needs.build-push-ecr.outputs.backend-image }}" \
           '.containerDefinitions[0].image = $IMAGE | del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .placementConstraints, .compatibilities, .registeredAt, .registeredBy)' \
           backend-task-def.json > backend-task-def-new.json
        
        # Update worker task definition
        jq --arg IMAGE "${{ needs.build-push-ecr.outputs.worker-image }}" \
           '.containerDefinitions[0].image = $IMAGE | del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .placementConstraints, .compatibilities, .registeredAt, .registeredBy)' \
           worker-task-def.json > worker-task-def-new.json

    - name: Register new task definitions
      run: |
        set -euo pipefail
        
        # Register new backend task definition
        BACKEND_TASK_DEF_ARN=$(aws ecs register-task-definition \
          --cli-input-json file://backend-task-def-new.json \
          --query 'taskDefinition.taskDefinitionArn' \
          --output text)
        
        # Register new worker task definition
        WORKER_TASK_DEF_ARN=$(aws ecs register-task-definition \
          --cli-input-json file://worker-task-def-new.json \
          --query 'taskDefinition.taskDefinitionArn' \
          --output text)
        
        echo "BACKEND_TASK_DEF_ARN=${BACKEND_TASK_DEF_ARN}" >> $GITHUB_ENV
        echo "WORKER_TASK_DEF_ARN=${WORKER_TASK_DEF_ARN}" >> $GITHUB_ENV

    - name: Update ECS services
      run: |
        set -euo pipefail
        
        # Update backend service
        aws ecs update-service \
          --cluster "${{ env.ECS_CLUSTER }}" \
          --service "${{ env.BACKEND_SERVICE }}" \
          --task-definition "${BACKEND_TASK_DEF_ARN}" \
          --force-new-deployment
        
        # Update worker service
        aws ecs update-service \
          --cluster "${{ env.ECS_CLUSTER }}" \
          --service "${{ env.WORKER_SERVICE }}" \
          --task-definition "${WORKER_TASK_DEF_ARN}" \
          --force-new-deployment

    - name: Wait for backend service to stabilize
      run: |
        set -euo pipefail
        
        echo "Waiting for backend service to reach stable state..."
        aws ecs wait services-stable \
          --cluster "${{ env.ECS_CLUSTER }}" \
          --services "${{ env.BACKEND_SERVICE }}" \
          --no-cli-pager
        
        echo "Backend service is now stable"

    - name: Wait for worker service to stabilize
      run: |
        set -euo pipefail
        
        echo "Waiting for worker service to reach stable state..."
        aws ecs wait services-stable \
          --cluster "${{ env.ECS_CLUSTER }}" \
          --services "${{ env.WORKER_SERVICE }}" \
          --no-cli-pager
        
        echo "Worker service is now stable"

    - name: Get ALB URL and test health endpoint
      id: health-check
      run: |
        set -euo pipefail
        
        # Check if we have domain configuration
        if [ "${{ vars.DOMAIN_NAME }}" != "" ] && [ "${{ vars.CLOUDFLARE_ZONE_ID }}" != "" ]; then
          # Use API subdomain for health check
          API_URL="https://api.${{ vars.DOMAIN_NAME }}/health"
          echo "ALB_URL=https://api.${{ vars.DOMAIN_NAME }}" >> $GITHUB_ENV
        else
          # Get ALB DNS name from ECS service
          ALB_DNS=$(aws elbv2 describe-load-balancers \
            --query 'LoadBalancers[?starts_with(LoadBalancerName, `clip-downloader`)].DNSName' \
            --output text)
          
          if [ -z "$ALB_DNS" ]; then
            echo "❌ Could not find ALB DNS name"
            exit 1
          fi
          
          API_URL="http://${ALB_DNS}/health"
          echo "ALB_URL=http://${ALB_DNS}" >> $GITHUB_ENV
        fi
        
        # Test health endpoint
        echo "Testing health endpoint at ${API_URL}"
        
        for i in {1..10}; do
          if curl -f -s "${API_URL}" > /dev/null; then
            echo "✅ Health check passed on attempt $i"
            echo "health_status=healthy" >> $GITHUB_OUTPUT
            break
          else
            echo "⏳ Health check failed on attempt $i, retrying in 30s..."
            if [ $i -eq 10 ]; then
              echo "❌ Health check failed after 10 attempts"
              echo "health_status=unhealthy" >> $GITHUB_OUTPUT
              exit 1
            fi
            sleep 30
          fi
        done

    - name: Add deployment summary
      if: always()
      run: |
        echo "## Deployment Results" >> $GITHUB_STEP_SUMMARY
        echo "### Service Updates" >> $GITHUB_STEP_SUMMARY
        echo "- **Backend Service**: \`${{ env.BACKEND_SERVICE }}\` ✅" >> $GITHUB_STEP_SUMMARY
        echo "- **Worker Service**: \`${{ env.WORKER_SERVICE }}\` ✅" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### New Task Definitions" >> $GITHUB_STEP_SUMMARY
        echo "- **Backend**: \`${BACKEND_TASK_DEF_ARN}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Worker**: \`${WORKER_TASK_DEF_ARN}\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Health Check" >> $GITHUB_STEP_SUMMARY
        if [ "${{ steps.health-check.outputs.health_status }}" = "healthy" ]; then
          echo "- **ALB Health**: ✅ \`${{ env.ALB_URL }}\`" >> $GITHUB_STEP_SUMMARY
        else
          echo "- **ALB Health**: ❌ \`${{ env.ALB_URL }}\`" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Images Deployed" >> $GITHUB_STEP_SUMMARY
        echo "- **Backend**: \`${{ needs.build-push-ecr.outputs.backend-image }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Worker**: \`${{ needs.build-push-ecr.outputs.worker-image }}\`" >> $GITHUB_STEP_SUMMARY 